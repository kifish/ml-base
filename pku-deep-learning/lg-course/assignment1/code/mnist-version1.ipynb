{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-0ab3ad50c7d4>:4: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /Users/k/Library/Python/3.6/lib/python/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /Users/k/Library/Python/3.6/lib/python/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From /Users/k/Library/Python/3.6/lib/python/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From /Users/k/Library/Python/3.6/lib/python/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/k/Library/Python/3.6/lib/python/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/k/Library/Python/3.6/lib/python/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "Training data shape:  (55000, 784)\n",
      "Training labels shape:  (55000, 10)\n",
      "Test data shape:  (10000, 784)\n",
      "Test labels shape:  (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "X_train = mnist.train.images\n",
    "y_train = mnist.train.labels\n",
    "X_test = mnist.test.images\n",
    "y_test = mnist.test.labels\n",
    "\n",
    "# As a sanity check, we print out the size of the training and test data.\n",
    "print('Training data shape: ', X_train.shape)    \n",
    "print('Training labels shape: ', y_train.shape)   \n",
    "print('Test data shape: ', X_test.shape)        \n",
    "print('Test labels shape: ', y_test.shape)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(y_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# Create the model\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "# W_1 = tf.Variable(tf.zeros([784, 30]))\n",
    "W_1 = tf.get_variable('W_1', [784, 30], initializer=tf.random_normal_initializer())\n",
    "# b_1 = tf.Variable(tf.zeros([30]))\n",
    "b_1 = tf.get_variable('b_1', [30], initializer=tf.random_normal_initializer())\n",
    "z_1 = tf.matmul(x, W_1) + b_1\n",
    "a_1 = tf.sigmoid(z_1)\n",
    "\n",
    "# W_2 = tf.Variable(tf.zeros([30, 10]))\n",
    "W_2 = tf.get_variable('W_2', [30, 10], initializer=tf.random_normal_initializer())\n",
    "# b_2 = tf.Variable(tf.zeros([10]))\n",
    "b_2 = tf.get_variable('b_2', [10], initializer=tf.random_normal_initializer())\n",
    "z_2 = tf.matmul(a_1, W_2) + b_2\n",
    "# a_2 = tf.sigmoid(z_2)\n",
    "a_2 = z_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss and optimizer\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "loss = tf.losses.mean_squared_error(y, a_2)\n",
    "# loss = tf.reduce_mean(tf.norm(y - a_2, axis=1)**2) / 2\n",
    "#换成交叉熵效果应该会好一点\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 training loss: 0.3375765188447847, test acc: 0.411\n",
      "Epoch 1 training loss: 0.08783605913535139, test acc: 0.4456\n",
      "Epoch 2 training loss: 0.08012529316936157, test acc: 0.4523\n",
      "Epoch 3 training loss: 0.07675954767359656, test acc: 0.4759\n",
      "Epoch 4 training loss: 0.0746275606700254, test acc: 0.4986\n",
      "Epoch 5 training loss: 0.07298653009129251, test acc: 0.4987\n",
      "Epoch 6 training loss: 0.07162792671313385, test acc: 0.5166\n",
      "Epoch 7 training loss: 0.07036911868444572, test acc: 0.5295\n",
      "Epoch 8 training loss: 0.06922890501850433, test acc: 0.5364\n",
      "Epoch 9 training loss: 0.06817276533510264, test acc: 0.5498\n",
      "Epoch 10 training loss: 0.06712568888284315, test acc: 0.556\n",
      "Epoch 11 training loss: 0.06612012190835678, test acc: 0.571\n",
      "Epoch 12 training loss: 0.0651567732446066, test acc: 0.5814\n",
      "Epoch 13 training loss: 0.06421142333921472, test acc: 0.583\n",
      "Epoch 14 training loss: 0.0632687595407829, test acc: 0.587\n",
      "Epoch 15 training loss: 0.06233272587549403, test acc: 0.6035\n",
      "Epoch 16 training loss: 0.06143476751469467, test acc: 0.6105\n",
      "Epoch 17 training loss: 0.06052002654438316, test acc: 0.6174\n",
      "Epoch 18 training loss: 0.05965366883400415, test acc: 0.6233\n",
      "Epoch 19 training loss: 0.05880270857107994, test acc: 0.6324\n",
      "Epoch 20 training loss: 0.058001618131572695, test acc: 0.6263\n",
      "Epoch 21 training loss: 0.05717436565673272, test acc: 0.6313\n",
      "Epoch 22 training loss: 0.05639107979330337, test acc: 0.6444\n",
      "Epoch 23 training loss: 0.05564917564887704, test acc: 0.6491\n",
      "Epoch 24 training loss: 0.0549547998061975, test acc: 0.6588\n",
      "Epoch 25 training loss: 0.0542886203486014, test acc: 0.6678\n",
      "Epoch 26 training loss: 0.053643702741635185, test acc: 0.6659\n",
      "Epoch 27 training loss: 0.05300357478549918, test acc: 0.6721\n",
      "Epoch 28 training loss: 0.052395210642014224, test acc: 0.6785\n",
      "Epoch 29 training loss: 0.0518209115889495, test acc: 0.6852\n"
     ]
    }
   ],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(a_2, 1), tf.argmax(y, 1)) #(num_training,1),预测正确为1，反之为0\n",
    "accuracy_op = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) # tf.int32 注意tf.int32会导致，acc算出来一直为0，reduce_mean可能要求输入为浮点数\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "# Train\n",
    "batch_size = 30 \n",
    "num_epoch = 30\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    num_train = X_train.shape[0]\n",
    "    num_batch = int(num_train/batch_size) + 1\n",
    "    for epoch in range(num_epoch):\n",
    "        idxs = np.arange(num_train) \n",
    "        np.random.shuffle(idxs)\n",
    "        avg_cost = 0\n",
    "        for batch_idx in range(num_batch):\n",
    "            start_idx = batch_idx * batch_size\n",
    "            end_idx = (batch_idx + 1) * batch_size \n",
    "            if end_idx > num_train:\n",
    "                end_idx = num_train\n",
    "            mask_idxs =  idxs[start_idx:end_idx]\n",
    "            batch_xs, batch_ys =  X_train[mask_idxs], y_train[mask_idxs]\n",
    "            _, c = sess.run([train_step,loss], feed_dict={x: batch_xs, y: batch_ys})\n",
    "            avg_cost += c / num_batch\n",
    "        #evaluate\n",
    "        acc = sess.run(accuracy_op,feed_dict={x: X_test,y: y_test})\n",
    "        print(\"Epoch %s training loss: %s, test acc: %s\" % (epoch, avg_cost,acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_training = 40000\n",
    "num_val = 10000\n",
    "# num_test = 10000 \n",
    "idxs = list(range(num_training + num_val))\n",
    "np.random.shuffle(idxs)\n",
    "X_train_all = X_train\n",
    "y_train_all = y_train\n",
    "X_train = X_train_all[idxs[:num_training]]\n",
    "y_train = y_train_all[idxs[:num_training]]\n",
    "X_val = X_train_all[idxs[num_training:]]\n",
    "y_val = y_train_all[idxs[num_training:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (40000, 32, 32, 3)\n",
      "Training labels shape:  (40000,)\n",
      "Val data shape:  (10000, 32, 32, 3)\n",
      "Val labels shape:  (10000,)\n",
      "Test data shape:  (10000, 32, 32, 3)\n",
      "Test labels shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "print('Training data shape: ', X_train.shape)     \n",
    "print('Training labels shape: ', y_train.shape) \n",
    "print('Val data shape: ', X_val.shape)        \n",
    "print('Val labels shape: ', y_val.shape)     \n",
    "print('Test data shape: ', X_test.shape)     \n",
    "print('Test labels shape: ', y_test.shape)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "print(type(y_train))\n",
    "print((y_train[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (40000, 3072)\n",
      "Training labels shape:  (40000, 10)\n",
      "Val data shape:  (10000, 3072)\n",
      "Val labels shape:  (10000, 10)\n",
      "Test data shape:  (10000, 3072)\n",
      "Test labels shape:  (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# reshape the image data into rows\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], -1))    \n",
    "X_val = np.reshape(X_val, (X_val.shape[0], -1))         \n",
    "X_test = np.reshape(X_test, (X_test.shape[0], -1))       \n",
    "# y_train = tf.one_hot(y_train, 10) #10 is the num_class\n",
    "# y_val = tf.one_hot(y_val, 10)\n",
    "# y_test = tf.one_hot(y_test,10)\n",
    "#warning: 这时候X是numpy.array,y是tensor!\n",
    "# 之后batch_ys = y_train[mask_idxs] 会导致报错，tensorflow.python.framework.errors_impl.InvalidArgumentError: Shape must be rank 1 but is rank 2 for 'strided_slice' (op: 'StridedSlice') with input shapes: [40000,10], [1,30], [1,30], [1].\n",
    "# 因为要启动session，才能操作tensor。\n",
    "#因此可以使用numpy进行onehot操作。\n",
    "y_train = np.eye(num_classes)[y_train]\n",
    "y_val = np.eye(num_classes)[y_val]\n",
    "y_test = np.eye(num_classes)[y_test]\n",
    "print('Training data shape: ', X_train.shape)     \n",
    "print('Training labels shape: ', y_train.shape) \n",
    "print('Val data shape: ', X_val.shape)        \n",
    "print('Val labels shape: ', y_val.shape)     \n",
    "print('Test data shape: ', X_test.shape)     \n",
    "print('Test labels shape: ', y_test.shape)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print((y_train[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subtract the mean image\n",
    "# mean_image = np.mean(X_train, axis=0)       # (1,3072)\n",
    "# X_train -= mean_image\n",
    "# X_val -= mean_image\n",
    "# X_test -= mean_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the mean image\n",
    "# plt.figure(figsize=(4, 4))\n",
    "# plt.imshow(mean_image.reshape((32, 32, 3)).astype('uint8'))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bias trick, extending the data\n",
    "# X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])    \n",
    "# X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])          \n",
    "# X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])  \n",
    "# print('Training data shape: ', X_train.shape)  \n",
    "# print('Val data shape: ', X_val.shape)\n",
    "# print('Test data shape: ', X_test.shape) \n",
    "#暂不需要用这个技巧。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "x = tf.placeholder(tf.float32, [None, 3072])\n",
    "# W_1 = tf.Variable(tf.zeros([3072, 30]))\n",
    "W_1 = tf.get_variable('W_1', [3072, 30], initializer=tf.random_normal_initializer())\n",
    "# b_1 = tf.Variable(tf.zeros([30]))\n",
    "b_1 = tf.get_variable('b_1', [30], initializer=tf.random_normal_initializer())\n",
    "z_1 = tf.matmul(x, W_1) + b_1\n",
    "a_1 = tf.sigmoid(z_1)\n",
    "\n",
    "# W_2 = tf.Variable(tf.zeros([30, 10]))\n",
    "W_2 = tf.get_variable('W_2', [30, 10], initializer=tf.random_normal_initializer())\n",
    "# b_2 = tf.Variable(tf.zeros([10]))\n",
    "b_2 = tf.get_variable('b_2', [10], initializer=tf.random_normal_initializer())\n",
    "z_2 = tf.matmul(a_1, W_2) + b_2\n",
    "# a_2 = tf.sigmoid(z_2)\n",
    "a_2 = z_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss and optimizer\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "loss = tf.losses.mean_squared_error(y, a_2)\n",
    "# loss = tf.reduce_mean(tf.norm(y - a_2, axis=1)**2) / 2\n",
    "#换成交叉熵效果应该会好一点\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 training loss: 0.26318674077136084, validation acc: 0.1018\n",
      "Epoch 1 training loss: 0.10105381104564552, validation acc: 0.0996\n",
      "Epoch 2 training loss: 0.09562247110784858, validation acc: 0.1051\n",
      "Epoch 3 training loss: 0.09481414134936711, validation acc: 0.0999\n",
      "Epoch 4 training loss: 0.0944979408423583, validation acc: 0.0995\n",
      "Epoch 5 training loss: 0.09407026828943875, validation acc: 0.0971\n",
      "Epoch 6 training loss: 0.09387198172654891, validation acc: 0.1037\n",
      "Epoch 7 training loss: 0.09371794439081495, validation acc: 0.0967\n",
      "Epoch 8 training loss: 0.09355301659057151, validation acc: 0.1031\n",
      "Epoch 9 training loss: 0.09340980228433182, validation acc: 0.103\n",
      "Epoch 10 training loss: 0.09329453752986323, validation acc: 0.0995\n",
      "Epoch 11 training loss: 0.09320469488804474, validation acc: 0.104\n",
      "Epoch 12 training loss: 0.0930670933711387, validation acc: 0.1026\n",
      "Epoch 13 training loss: 0.09299058335898863, validation acc: 0.1033\n",
      "Epoch 14 training loss: 0.09285095341872245, validation acc: 0.106\n",
      "Epoch 15 training loss: 0.09274219633868976, validation acc: 0.1036\n",
      "Epoch 16 training loss: 0.09269182747048405, validation acc: 0.1038\n",
      "Epoch 17 training loss: 0.09258394897252194, validation acc: 0.0971\n",
      "Epoch 18 training loss: 0.09253882985377765, validation acc: 0.1004\n",
      "Epoch 19 training loss: 0.09244899289204982, validation acc: 0.1002\n",
      "Epoch 20 training loss: 0.09239372615513967, validation acc: 0.1032\n",
      "Epoch 21 training loss: 0.09231300590032035, validation acc: 0.0996\n",
      "Epoch 22 training loss: 0.09224455460146447, validation acc: 0.0995\n",
      "Epoch 23 training loss: 0.09219511927872057, validation acc: 0.1031\n",
      "Epoch 24 training loss: 0.09214747748498267, validation acc: 0.0996\n",
      "Epoch 25 training loss: 0.09209597099578005, validation acc: 0.1036\n",
      "Epoch 26 training loss: 0.09206261872865458, validation acc: 0.1062\n",
      "Epoch 27 training loss: 0.09202660621888087, validation acc: 0.1039\n",
      "Epoch 28 training loss: 0.09196516246474849, validation acc: 0.0995\n",
      "Epoch 29 training loss: 0.09191445373501367, validation acc: 0.1036\n",
      "Epoch 30 training loss: 0.09187918616601197, validation acc: 0.1053\n",
      "Epoch 31 training loss: 0.09184117464982598, validation acc: 0.0996\n",
      "Epoch 32 training loss: 0.09181866114554184, validation acc: 0.1001\n",
      "Epoch 33 training loss: 0.09180688463162626, validation acc: 0.1032\n",
      "Epoch 34 training loss: 0.0917351631757649, validation acc: 0.107\n",
      "Epoch 35 training loss: 0.09171441463248126, validation acc: 0.0996\n",
      "Epoch 36 training loss: 0.09167573140791704, validation acc: 0.1035\n",
      "Epoch 37 training loss: 0.09164062375272573, validation acc: 0.1035\n",
      "Epoch 38 training loss: 0.09163039947847373, validation acc: 0.1038\n",
      "Epoch 39 training loss: 0.09158835746627159, validation acc: 0.105\n",
      "Epoch 40 training loss: 0.09153490408562438, validation acc: 0.0978\n",
      "Epoch 41 training loss: 0.09152171935053953, validation acc: 0.1023\n",
      "Epoch 42 training loss: 0.09146164608435112, validation acc: 0.1061\n",
      "Epoch 43 training loss: 0.09146794176910407, validation acc: 0.1054\n",
      "Epoch 44 training loss: 0.09144324242979329, validation acc: 0.0992\n",
      "Epoch 45 training loss: 0.09137129027960543, validation acc: 0.1037\n",
      "Epoch 46 training loss: 0.09136122214360747, validation acc: 0.0986\n",
      "Epoch 47 training loss: 0.09135647468280043, validation acc: 0.0996\n",
      "Epoch 48 training loss: 0.0913405458832431, validation acc: 0.1076\n",
      "Epoch 49 training loss: 0.09127027479403918, validation acc: 0.0998\n",
      "Epoch 50 training loss: 0.09113578040940284, validation acc: 0.1253\n",
      "Epoch 51 training loss: 0.09107063269463256, validation acc: 0.1022\n",
      "Epoch 52 training loss: 0.09103390163388743, validation acc: 0.1038\n",
      "Epoch 53 training loss: 0.09094518931328435, validation acc: 0.1208\n",
      "Epoch 54 training loss: 0.09081571074998555, validation acc: 0.1021\n",
      "Epoch 55 training loss: 0.0907450463047538, validation acc: 0.1251\n",
      "Epoch 56 training loss: 0.09066937471749735, validation acc: 0.1432\n",
      "Epoch 57 training loss: 0.09059477172445694, validation acc: 0.1302\n",
      "Epoch 58 training loss: 0.0906509481415131, validation acc: 0.1022\n",
      "Epoch 59 training loss: 0.09047662841527535, validation acc: 0.121\n",
      "Epoch 60 training loss: 0.0904396526694208, validation acc: 0.1426\n",
      "Epoch 61 training loss: 0.09054962217226911, validation acc: 0.1011\n",
      "Epoch 62 training loss: 0.0906555857760705, validation acc: 0.1213\n",
      "Epoch 63 training loss: 0.09045703735934195, validation acc: 0.1267\n",
      "Epoch 64 training loss: 0.09013753903770005, validation acc: 0.0982\n",
      "Epoch 65 training loss: 0.09012371981921405, validation acc: 0.1573\n",
      "Epoch 66 training loss: 0.0901194930389367, validation acc: 0.1481\n",
      "Epoch 67 training loss: 0.08997977283159107, validation acc: 0.1439\n",
      "Epoch 68 training loss: 0.08994188798086047, validation acc: 0.162\n",
      "Epoch 69 training loss: 0.0897750969490279, validation acc: 0.1299\n",
      "Epoch 70 training loss: 0.08973103885104616, validation acc: 0.1462\n",
      "Epoch 71 training loss: 0.08973138293494352, validation acc: 0.1358\n",
      "Epoch 72 training loss: 0.08963623499941789, validation acc: 0.1271\n",
      "Epoch 73 training loss: 0.08943737365830434, validation acc: 0.1451\n",
      "Epoch 74 training loss: 0.08947772391397371, validation acc: 0.1645\n",
      "Epoch 75 training loss: 0.08944636944686993, validation acc: 0.1471\n",
      "Epoch 76 training loss: 0.08950949428991227, validation acc: 0.1483\n",
      "Epoch 77 training loss: 0.08932852957291587, validation acc: 0.1675\n",
      "Epoch 78 training loss: 0.08928211813588728, validation acc: 0.1371\n",
      "Epoch 79 training loss: 0.08922503669669427, validation acc: 0.154\n",
      "Epoch 80 training loss: 0.08931356443949547, validation acc: 0.1486\n",
      "Epoch 81 training loss: 0.08916492252491955, validation acc: 0.1676\n",
      "Epoch 82 training loss: 0.08897184475563812, validation acc: 0.1672\n",
      "Epoch 83 training loss: 0.08898268235677004, validation acc: 0.1393\n",
      "Epoch 84 training loss: 0.08889361723386184, validation acc: 0.1289\n",
      "Epoch 85 training loss: 0.08893697418976226, validation acc: 0.159\n",
      "Epoch 86 training loss: 0.08899672121099986, validation acc: 0.1635\n",
      "Epoch 87 training loss: 0.08941780166446356, validation acc: 0.1332\n",
      "Epoch 88 training loss: 0.08938306925826713, validation acc: 0.1415\n",
      "Epoch 89 training loss: 0.08921850369922059, validation acc: 0.1169\n",
      "Epoch 90 training loss: 0.0896632031969044, validation acc: 0.1531\n",
      "Epoch 91 training loss: 0.08910825220809357, validation acc: 0.1563\n",
      "Epoch 92 training loss: 0.08879048222008476, validation acc: 0.1256\n",
      "Epoch 93 training loss: 0.08925964923455443, validation acc: 0.1195\n",
      "Epoch 94 training loss: 0.09009813543582235, validation acc: 0.1153\n",
      "Epoch 95 training loss: 0.08917635082193216, validation acc: 0.158\n",
      "Epoch 96 training loss: 0.08878803496686047, validation acc: 0.1712\n",
      "Epoch 97 training loss: 0.08862405337918287, validation acc: 0.1259\n",
      "Epoch 98 training loss: 0.08889050667231417, validation acc: 0.1654\n",
      "Epoch 99 training loss: 0.08869597129244507, validation acc: 0.1463\n"
     ]
    }
   ],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(a_2, 1), tf.argmax(y, 1)) #(num_training,1),预测正确为1，反之为0\n",
    "accuracy_op = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) # tf.int32 注意tf.int32会导致，acc算出来一直为0，reduce_mean可能要求输入为浮点数\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "# Train\n",
    "batch_size = 30 \n",
    "num_epoch = 100\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    num_train = X_train.shape[0]\n",
    "    num_batch = int(num_train/batch_size) + 1\n",
    "    for epoch in range(num_epoch):\n",
    "        idxs = np.arange(num_train) \n",
    "        np.random.shuffle(idxs)\n",
    "        avg_cost = 0\n",
    "        for batch_idx in range(num_batch):\n",
    "            start_idx = batch_idx * batch_size\n",
    "            end_idx = (batch_idx + 1) * batch_size \n",
    "            if end_idx > num_train:\n",
    "                end_idx = num_train\n",
    "            mask_idxs =  idxs[start_idx:end_idx]\n",
    "            batch_xs, batch_ys =  X_train[mask_idxs], y_train[mask_idxs]\n",
    "            _, c = sess.run([train_step,loss], feed_dict={x: batch_xs, y: batch_ys})\n",
    "            avg_cost += c / num_batch\n",
    "        #evaluate\n",
    "        acc = sess.run(accuracy_op,feed_dict={x: X_val,y: y_val})\n",
    "        print(\"Epoch %s training loss: %s, validation acc: %s\" % (epoch, avg_cost,acc))\n",
    "#         print(\"Epoch %s validation acc: %s \" % (epoch, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
